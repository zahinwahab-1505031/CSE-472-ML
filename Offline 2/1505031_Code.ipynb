{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "__t5WUP1FB-u"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import collections\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "Qsg6ja00FJ_A",
    "outputId": "9936a92a-69bc-4d1d-aabb-798f0adbd48e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Zahin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Zahin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Zahin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "p6RPuAsGFP5Z",
    "outputId": "ca5adfff-1f4d-49d8-c438-b1b6925d1f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Q4T3frrEFvYU",
    "outputId": "fa4428dc-134c-4428-ef12-3a11f6d556e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anime', 'Arduino', 'Astronomy', 'Biology', 'Chess', 'Coffee', 'Cooking', 'Law', 'Space', 'Windows_Phone', 'Wood_Working']\n"
     ]
    }
   ],
   "source": [
    "topics = []\n",
    "def getTopicnames():\n",
    "    with open('Data/topics_all.txt', 'r') as reader:\n",
    "        line = reader.readline()\n",
    "        while line != '':  # The EOF char is an empty string\n",
    "            if line[-1] == \"\\n\":\n",
    "\n",
    "              topics.append(line[:-1])\n",
    "            else:\n",
    "              topics.append(line)\n",
    "            line = reader.readline()\n",
    "\n",
    "\n",
    "\n",
    "getTopicnames()\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_mxyLe5EF3-A"
   },
   "outputs": [],
   "source": [
    "def preprocessText(raw_text):\n",
    "    #remove html tags\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    text = re.sub(cleanr, '', raw_text)\n",
    "    #remove any URLs\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    #Remove punctuations\n",
    "    text=text.translate((str.maketrans('','',string.punctuation)))\n",
    "    #Number Removal\n",
    "    text = re.sub(r'[-+]?\\d+', '', text)\n",
    "    #removing unicode\n",
    "    text = re.sub(r'[^\\x00-\\x7F]',' ', text) # \n",
    "    text = text.lower()\n",
    "    #Tokenize\n",
    "    text = word_tokenize(text)\n",
    "    #Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    #Lemmatize tokens\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    #Stemming tokens\n",
    "    stemmer= PorterStemmer()\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "ph628pTjHq7S",
    "outputId": "4d8ac758-60ba-44a1-b350-f4ec60992b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime\n",
      "Arduino\n",
      "Astronomy\n",
      "Biology\n",
      "Chess\n",
      "Coffee\n",
      "Cooking\n",
      "Law\n",
      "Space\n",
      "Windows_Phone\n",
      "Wood_Working\n",
      "{1: 'Anime', 2: 'Arduino', 3: 'Astronomy', 4: 'Biology', 5: 'Chess', 6: 'Coffee', 7: 'Cooking', 8: 'Law', 9: 'Space', 10: 'Windows_Phone', 11: 'Wood_Working'}\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "X_validation = []\n",
    "Y_validation = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "topic_dict = {}\n",
    "topic_words_dict = {} # dict()\n",
    "import random\n",
    "prefix_for_colab =  \"\" #\"/content/drive/My Drive/CSE_472_Assignment2_dataset/\"\n",
    "def preprocessData():\n",
    "    list_of_words = []\n",
    "    iter = 1\n",
    "    count_train = 0\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "        list_of_words_in_topic = []\n",
    "        topic_dict[iter] = topic\n",
    "        topic_words_dict[iter] = []\n",
    "\n",
    "        file_name = prefix_for_colab+\"Data/Training/\" + topic + \".xml\"\n",
    "        with open(file_name,'r',encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                soup = bs(content)\n",
    "                rows = soup.findAll(\"row\")\n",
    "                train_list_to_iterate = rows[:500]\n",
    "                validation_list_to_iterate = rows[500:700]\n",
    "                test_list_to_iterate = rows[700:1200]\n",
    "                for item in train_list_to_iterate:\n",
    "                    text = item[\"body\"]\n",
    "                    if text == \"\":\n",
    "                      continue\n",
    "                    processed_text = preprocessText(text)\n",
    "                    X_train.append(np.array(processed_text))\n",
    "                    Y_train.append(iter)\n",
    "                    list_of_words.append(processed_text)\n",
    "                    list_of_words_in_topic.append(processed_text)\n",
    "                for item in validation_list_to_iterate:\n",
    "                    text = item[\"body\"]\n",
    "                    if text == \"\":\n",
    "                      continue\n",
    "                    processed_text = preprocessText(text)\n",
    "                    X_validation.append(np.array(processed_text))\n",
    "                    Y_validation.append(iter)\n",
    "                for i in range(50):\n",
    "                  temp_X = []\n",
    "                  temp_Y = []\n",
    "                  for item in test_list_to_iterate[i*10:(i+1)*10]:\n",
    "                      text = item[\"body\"]\n",
    "                      if text == \"\":\n",
    "                        continue\n",
    "                      processed_text = preprocessText(text)\n",
    "                      temp_X.append(np.array(processed_text))\n",
    "                      temp_Y.append(iter)\n",
    "                  \n",
    "                  #print(temp_Y)\n",
    "                  X_test.append(temp_X)\n",
    "                  Y_test.append(temp_Y)\n",
    "                  #print(Y_test)\n",
    "\n",
    "        temp = sum(list_of_words_in_topic, []) \n",
    "        topic_words_dict[iter] = temp\n",
    "        iter = iter + 1\n",
    "    vocabulary_raw = sum(list_of_words, []) \n",
    "    return vocabulary_raw\n",
    "\n",
    "      \n",
    "vocabulary_raw = preprocessData()\n",
    "print(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "M2wdq3ipPHZ1",
    "outputId": "eb95f6a6-01f7-441c-ed03-9e0e9d74098a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19725\n"
     ]
    }
   ],
   "source": [
    "def makeDictionary():\n",
    "    import collections\n",
    "    counts = collections.Counter(vocabulary_raw)\n",
    "    return dict(counts)\n",
    "feature_space = makeDictionary()\n",
    "unique_words_in_feature_space = np.sort(np.array(list(feature_space.keys())).reshape(1,-1)[0])\n",
    "print(len(unique_words_in_feature_space))\n",
    "modV = len(unique_words_in_feature_space)\n",
    "total_words = np.sum(np.array(list(feature_space.values())).reshape(1,-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "f8Q3az-H6elj"
   },
   "outputs": [],
   "source": [
    "Y_test_for_each_iteration = {}\n",
    "for i in range(50):\n",
    "  Y_test_for_each_iteration[i] = []\n",
    "  temp = []\n",
    "  for j in range(len(topics)):\n",
    "    temp.append (Y_test[j*50:(j+1)*50][i])\n",
    "  temp2 = sum(temp, []) \n",
    "  Y_test_for_each_iteration[i] = temp2\n",
    "X_test_for_each_iteration = {}\n",
    "for i in range(50):\n",
    "  X_test_for_each_iteration[i] = []\n",
    "  temp = []\n",
    "  for j in range(len(topics)):\n",
    "    temp.append (X_test[j*50:(j+1)*50][i])\n",
    "  temp2 = sum(temp, []) \n",
    "  X_test_for_each_iteration[i] = temp2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9zFsNkhW9-CR"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def getBooleanVector(string_tokens):\n",
    "  boolean_vector = np.zeros((len(string_tokens),len(feature_space)))\n",
    "  words_in_feature_space = np.sort(np.array(list(feature_space.keys())).reshape(1,-1)[0])\n",
    " # print(words_in_feature_space.shape)\n",
    "  for i in range(len(string_tokens)):\n",
    "     boolean_vector[i] = np.in1d(words_in_feature_space, string_tokens[i])\n",
    "  return boolean_vector\n",
    "X_test_boolean_vector_reprenstation = {}\n",
    "for i in range(50):\n",
    "  X_test_boolean_vector_reprenstation[i]=getBooleanVector(X_test_for_each_iteration[i])\n",
    "\n",
    "X_train_boolean_vector_reprenstation=getBooleanVector(X_train)\n",
    "X_validation_boolean_vector_reprenstation=getBooleanVector(X_validation)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "is-rJxekBNJa"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6dce85002400>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mX_train_numeric_vector_reprenstation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetNumericVector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mX_validation_numeric_vector_reprenstation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetNumericVector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-6dce85002400>\u001b[0m in \u001b[0;36mgetNumericVector\u001b[1;34m(string_tokens)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetNumericVector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m   \u001b[0mnumeric_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m   \u001b[0mwords_in_feature_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m       \u001b[0marr1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords_in_feature_space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def getNumericVector(string_tokens):\n",
    "  numeric_vector = np.zeros((len(string_tokens),len(feature_space)))\n",
    "  words_in_feature_space = np.sort(np.array(list(feature_space.keys())).reshape(1,-1)[0])\n",
    "  for i in range(len(string_tokens)):\n",
    "      arr1 = words_in_feature_space\n",
    "      arr2 = np.array(string_tokens[i])\n",
    "      temp = np.in1d(arr1,arr2)\n",
    "      idx = np.searchsorted(arr1,arr2)\n",
    "      idx[idx==len(arr1)] = 0\n",
    "      mask = arr1[idx]==arr2\n",
    "      out = np.bincount(idx[mask])\n",
    "      temp = temp.astype('int64') # implicitly changes all the \"False\" values to 0\n",
    "      np.putmask(temp, temp, out)\n",
    "      numeric_vector[i] = temp\n",
    "  return numeric_vector\n",
    "X_test_numeric_vector_reprenstation = {}\n",
    "for i in range(50):\n",
    "  X_test_numeric_vector_reprenstation[i]=getNumericVector(X_test_for_each_iteration[i])\n",
    "\n",
    "X_train_numeric_vector_reprenstation=getNumericVector(X_train)\n",
    "X_validation_numeric_vector_reprenstation=getNumericVector(X_validation)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "w-Rj5meODYOA"
   },
   "outputs": [],
   "source": [
    "def getTFIDFVector(string_tokens,boolean_vector,numeric_vector):\n",
    "  #np.seterr(divide='ignore', invalid='ignore')\n",
    "  tf_idf_vector = np.zeros((len(string_tokens),len(feature_space)))\n",
    "  words_in_feature_space = np.sort(np.array(list(feature_space.keys())).reshape(1,-1)[0])\n",
    "  for i in range(len(string_tokens)):\n",
    "      D = len(string_tokens)\n",
    "      C_w = np.sum(boolean_vector,axis = 0).reshape(1,-1)\n",
    "      W_d = np.sum(numeric_vector[i])\n",
    "      if W_d == 0:\n",
    "        W_d = 0.0000000000000000000000001\n",
    "      tf = numeric_vector[i]/W_d \n",
    "      alpha = 1 #0.000000000000000001\n",
    "      beta  = 1 #0.000000000000000001\n",
    "      idf = np.log( np.divide(D+alpha,C_w+beta))\n",
    "      #print(tf)\n",
    "      #print(idf) \n",
    "\n",
    "     # print(tf*idf)\n",
    "      tf_idf_vector[i] = tf*idf\n",
    "  return tf_idf_vector\n",
    "X_test_TFIDF_vector_reprenstation = {}\n",
    "for i in range(50):\n",
    "  X_test_TFIDF_vector_reprenstation[i]=getTFIDFVector(X_test_for_each_iteration[i],X_test_boolean_vector_reprenstation[i],X_test_numeric_vector_reprenstation[i])\n",
    "\n",
    "X_train_TFIDF_vector_reprenstation=getTFIDFVector(X_train,X_train_boolean_vector_reprenstation,X_train_numeric_vector_reprenstation)\n",
    "X_validation_TFIDF_reprenstation=getTFIDFVector(X_validation,X_validation_boolean_vector_reprenstation,X_validation_numeric_vector_reprenstation)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "mZ2ARi8tDdCR"
   },
   "outputs": [],
   "source": [
    "def hamming_distance(instance1, instance2):\n",
    "  return np.sum(np.logical_xor(instance1,instance2).astype(int),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "hncrUIV0DvbC"
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(instance1, instance2):\n",
    "  return np.linalg.norm(instance1 - instance2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "rvNghWzNDxxg"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(instance1, instance2):\n",
    "  numerator = np.sum(np.multiply(instance1,instance2),axis=1)\n",
    "  denominator = np.multiply(np.linalg.norm(instance1,axis=1),np.linalg.norm(instance2,axis=1))\n",
    "  return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "zRA8SKDzDz1V"
   },
   "outputs": [],
   "source": [
    "def prediction_knn(X_train, Y_train, X_test, version, n_neighbors=3):\n",
    "    allTestNeighbers=[]\n",
    "    allPredictedOutputs =[]\n",
    "    Y_train = np.array(Y_train)\n",
    "    #Determine Number of unique class lebels\n",
    "    if version == 'v2':\n",
    "      distances = euclidean_distance(X_train,X_test).reshape(1,-1)\n",
    "    elif version == 'v1':\n",
    "      distances = hamming_distance(X_train,X_test).reshape(1,-1)\n",
    "    elif version == 'v3':\n",
    "      distances = cosine_similarity(X_train,X_test).reshape(1,-1)\n",
    "    sorted_indices = np.argsort(distances) #,ascending=True)\n",
    "    \n",
    "  #  print(Y_train[sorted_indices].reshape(1,-1))\n",
    "    sortedYtrain = Y_train[sorted_indices].reshape(1,-1)[0][0:n_neighbors]\n",
    "    if version == 'v3':\n",
    "      sortedYtrain = Y_train[sorted_indices].reshape(1,-1)[0][-n_neighbors:]\n",
    "    (values,counts) = np.unique(sortedYtrain,return_counts=True)\n",
    "    ind=np.argmax(counts)\n",
    "    #print (values[ind])\n",
    "    predOut = values[ind]\n",
    "    return predOut\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "OkcxPPomD3vv"
   },
   "outputs": [],
   "source": [
    "def performanceEvaluation_knn(X_train, Y_train, X_test, Y_test, version, n_neighbors=3):\n",
    "    print(\"K=\",n_neighbors)\n",
    "    totalCount = 0\n",
    "    correctCount = 0\n",
    "    for testInput, testActualOutput in zip(X_test, Y_test):\n",
    "      predictedOutput = prediction_knn(X_train, Y_train, [testInput],version, n_neighbors)\n",
    "      if predictedOutput == testActualOutput:\n",
    "            correctCount += 1\n",
    "      totalCount += 1\n",
    "    \n",
    "    print(\"Total Correct Count: \",correctCount,\" Total Wrong Count: \",totalCount-correctCount,\" Accuracy: \",(correctCount*100)/(totalCount))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "ylS54KeF7mxS",
    "outputId": "5e127b8d-e9e0-4e11-b1ae-fc2e19d9b970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================Hamming============================================================\n",
      "K= 1\n",
      "Total Correct Count:  858  Total Wrong Count:  1249  Accuracy:  40.72140484100617\n",
      "K= 3\n",
      "Total Correct Count:  831  Total Wrong Count:  1276  Accuracy:  39.43996203132416\n",
      "K= 5\n",
      "Total Correct Count:  802  Total Wrong Count:  1305  Accuracy:  38.06359753203607\n",
      "=========================Euclidean============================================================\n",
      "K= 1\n",
      "Total Correct Count:  1189  Total Wrong Count:  918  Accuracy:  56.43094447081158\n",
      "K= 3\n",
      "Total Correct Count:  1146  Total Wrong Count:  961  Accuracy:  54.390128144280965\n",
      "K= 5\n",
      "Total Correct Count:  1130  Total Wrong Count:  977  Accuracy:  53.63075462743237\n",
      "=========================TF-IDF============================================================\n",
      "K= 1\n",
      "Total Correct Count:  1705  Total Wrong Count:  402  Accuracy:  80.92074038917893\n",
      "K= 3\n",
      "Total Correct Count:  1726  Total Wrong Count:  381  Accuracy:  81.91741813004272\n",
      "K= 5\n"
     ]
    }
   ],
   "source": [
    "print(\"=========================Hamming============================================================\")\n",
    "performanceEvaluation_knn(X_train_boolean_vector_reprenstation, Y_train, X_validation_boolean_vector_reprenstation, Y_validation,'v1',1)\n",
    "performanceEvaluation_knn(X_train_boolean_vector_reprenstation, Y_train, X_validation_boolean_vector_reprenstation, Y_validation,'v1',3)\n",
    "performanceEvaluation_knn(X_train_boolean_vector_reprenstation, Y_train, X_validation_boolean_vector_reprenstation, Y_validation,'v1',5)\n",
    "print(\"=========================Euclidean============================================================\")\n",
    "performanceEvaluation_knn(X_train_numeric_vector_reprenstation, Y_train, X_validation_numeric_vector_reprenstation, Y_validation,'v2',1) \n",
    "performanceEvaluation_knn(X_train_numeric_vector_reprenstation, Y_train, X_validation_numeric_vector_reprenstation, Y_validation,'v2',3) \n",
    "performanceEvaluation_knn(X_train_numeric_vector_reprenstation, Y_train, X_validation_numeric_vector_reprenstation, Y_validation,'v2',5) \n",
    "print(\"=========================TF-IDF============================================================\")\n",
    "performanceEvaluation_knn(X_train_TFIDF_vector_reprenstation, Y_train, X_validation_TFIDF_reprenstation, Y_validation,'v3',1) \n",
    "performanceEvaluation_knn(X_train_TFIDF_vector_reprenstation, Y_train, X_validation_TFIDF_reprenstation, Y_validation,'v3',3) \n",
    "performanceEvaluation_knn(X_train_TFIDF_vector_reprenstation, Y_train, X_validation_TFIDF_reprenstation, Y_validation,'v3',5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yBUI2cxbGogU",
    "outputId": "67649eac-08a8-446e-dda7-f791b79d0579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Correct Count:  22  Total Wrong Count:  8  Accuracy:  73.33333333333333\n",
      "=========================Euclidean============================================================\n",
      "K= 5\n",
      "Total Correct Count:  26  Total Wrong Count:  4  Accuracy:  86.66666666666667\n",
      "=========================TF-IDF============================================================\n",
      "K= 5\n",
      "Total Correct Count:  29  Total Wrong Count:  1  Accuracy:  96.66666666666667\n",
      "==========================FOR  18 th iteration\n",
      "=========================Hamiltonian============================================================\n",
      "K= 5\n",
      "Total Correct Count:  24  Total Wrong Count:  6  Accuracy:  80.0\n",
      "=========================Euclidean============================================================\n",
      "K= 5\n",
      "Total Correct Count:  25  Total Wrong Count:  5  Accuracy:  83.33333333333333\n",
      "=========================TF-IDF============================================================\n",
      "K= 5\n",
      "Total Correct Count:  29  Total Wrong Count:  1  Accuracy:  96.66666666666667\n",
      "==========================FOR  19 th iteration\n",
      "=========================Hamiltonian============================================================\n",
      "K= 5\n",
      "Total Correct Count:  20  Total Wrong Count:  10  Accuracy:  66.66666666666667\n",
      "=========================Euclidean============================================================\n",
      "K= 5\n",
      "Total Correct Count:  19  Total Wrong Count:  11  Accuracy:  63.333333333333336\n",
      "=========================TF-IDF============================================================\n",
      "K= 5\n",
      "Total Correct Count:  28  Total Wrong Count:  2  Accuracy:  93.33333333333333\n",
      "==========================FOR  20 th iteration\n",
      "=========================Hamiltonian============================================================\n",
      "K= 5\n",
      "Total Correct Count:  22  Total Wrong Count:  8  Accuracy:  73.33333333333333\n",
      "=========================Euclidean============================================================\n",
      "K= 5\n",
      "Total Correct Count:  26  Total Wrong Count:  4  Accuracy:  86.66666666666667\n",
      "=========================TF-IDF============================================================\n",
      "K= 5\n",
      "Total Correct Count:  29  Total Wrong Count:  1  Accuracy:  96.66666666666667\n",
      "==========================FOR  21 th iteration\n",
      "=========================Hamiltonian============================================================\n",
      "K= 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-f463e0edc6bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m#performanceEvaluation_knn(X_train_boolean_vector_reprenstation, Y_train, X_test_boolean_vector_reprenstation[i], Y_test_for_each_iteration[i],'v1',1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#performanceEvaluation_knn(X_train_boolean_vector_reprenstation, Y_train, X_test_boolean_vector_reprenstation[i], Y_test_for_each_iteration[i],'v1',3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mperformanceEvaluation_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_boolean_vector_reprenstation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_boolean_vector_reprenstation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test_for_each_iteration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'v1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-a5a663fb4a50>\u001b[0m in \u001b[0;36mperformanceEvaluation_knn\u001b[0;34m(X_train, Y_train, X_test, Y_test, version, n_neighbors)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcorrectCount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtestInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestActualOutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mpredictedOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtestInput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mpredictedOutput\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtestActualOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mcorrectCount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-568e29e493d4>\u001b[0m in \u001b[0;36mprediction_knn\u001b[0;34m(X_train, Y_train, X_test, version, n_neighbors)\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'v1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhamming_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'v3'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-0716fce86c08>\u001b[0m in \u001b[0;36mhamming_distance\u001b[0;34m(instance1, instance2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhamming_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_xor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minstance2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "  print(\"==========================FOR \",i+1,\"th iteration\")\n",
    "  print(\"=========================Hamiltonian============================================================\")\n",
    "  #performanceEvaluation_knn(X_train_boolean_vector_reprenstation, Y_train, X_test_boolean_vector_reprenstation[i], Y_test_for_each_iteration[i],'v1',1)\n",
    "  #performanceEvaluation_knn(X_train_boolean_vector_reprenstation, Y_train, X_test_boolean_vector_reprenstation[i], Y_test_for_each_iteration[i],'v1',3)\n",
    "  performanceEvaluation_knn(X_train_boolean_vector_reprenstation, Y_train, X_test_boolean_vector_reprenstation[i], Y_test_for_each_iteration[i],'v1',5)\n",
    "\n",
    "          \n",
    "  print(\"=========================Euclidean============================================================\")\n",
    "  #performanceEvaluation_knn(X_train_numeric_vector_reprenstation, Y_train, X_test_numeric_vector_reprenstation[i], Y_test_for_each_iteration[i],'v2',1) \n",
    "  #performanceEvaluation_knn(X_train_numeric_vector_reprenstation, Y_train, X_test_numeric_vector_reprenstation[i], Y_test_for_each_iteration[i],'v2',3) \n",
    "  performanceEvaluation_knn(X_train_numeric_vector_reprenstation, Y_train, X_test_numeric_vector_reprenstation[i], Y_test_for_each_iteration[i],'v2',5) \n",
    "\n",
    "  print(\"=========================TF-IDF============================================================\")\n",
    " #performanceEvaluation_knn(X_train_TFIDF_vector_reprenstation, Y_train, X_test_TFIDF_vector_reprenstation[i], Y_test_for_each_iteration[i],'v3',1) \n",
    "  #performanceEvaluation_knn(X_train_TFIDF_vector_reprenstation, Y_train, X_test_TFIDF_vector_reprenstation[i], Y_test_for_each_iteration[i],'v3',3) \n",
    "  performanceEvaluation_knn(X_train_TFIDF_vector_reprenstation, Y_train, X_test_TFIDF_vector_reprenstation[i], Y_test_for_each_iteration[i],'v3',5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MhzGxIQ9EgY_"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def prediction_Naive_Bayes(X_train, Y_train, X_test,alpha):\n",
    "  unique_labels, counts_labels = np.unique(Y_train, return_counts=True)\n",
    "  predictedOutput = -1\n",
    "  prob = -99999999999999\n",
    "  for label in unique_labels:\n",
    "   # print(\"For label: \",label)\n",
    "    words_in_this_topic = topic_words_dict[label]\n",
    "    counts = collections.Counter(words_in_this_topic)\n",
    "    word_counts_in_this_topic = dict(counts)\n",
    "    P_cm_dt = math.log(counts_labels[label-1]/len(Y_train))\n",
    "    N_cm = len(words_in_this_topic)\n",
    "    for word in X_test[0]:\n",
    "      N_wj_cm = 0\n",
    "      if word in words_in_this_topic:\n",
    "        N_wj_cm = word_counts_in_this_topic[word]\n",
    "      P_wj_given_cm = (N_wj_cm + alpha) /(N_cm + alpha*modV)\n",
    "      P_cm_dt = P_cm_dt+ math.log(P_wj_given_cm)\n",
    "    if P_cm_dt > prob:\n",
    "      prob = P_cm_dt\n",
    "      predictedOutput = label\n",
    "\n",
    "  return predictedOutput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "F8SPrhL6FLL6"
   },
   "outputs": [],
   "source": [
    "def performanceEvaluation_NB(X_train, Y_train, X_test, Y_test,alpha):\n",
    "    print(\"Smoothing factor: \",alpha)\n",
    "    totalCount = 0\n",
    "    correctCount = 0\n",
    "    for testInput, testActualOutput in zip(X_test, Y_test):\n",
    "      predictedOutput = prediction_Naive_Bayes(X_train, Y_train, [testInput],alpha)\n",
    "      \n",
    "      if predictedOutput == testActualOutput:\n",
    "            correctCount += 1\n",
    "      totalCount += 1\n",
    "    \n",
    "    print(\"Total Correct Count: \",correctCount,\" Total Wrong Count: \",totalCount-correctCount,\" Accuracy: \",(correctCount*100)/(totalCount))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "aPProZ6g9-wV",
    "outputId": "af353bec-ce28-4adb-fd13-6a034c45c54d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  1935  Total Wrong Count:  172  Accuracy:  91.83673469387755\n",
      "Smoothing factor:  0.001\n",
      "Total Correct Count:  1919  Total Wrong Count:  188  Accuracy:  91.07736117702895\n",
      "Smoothing factor:  0.0001\n",
      "Total Correct Count:  1898  Total Wrong Count:  209  Accuracy:  90.08068343616516\n",
      "Smoothing factor:  1\n",
      "Total Correct Count:  1919  Total Wrong Count:  188  Accuracy:  91.07736117702895\n",
      "Smoothing factor:  10\n",
      "Total Correct Count:  1798  Total Wrong Count:  309  Accuracy:  85.33459895586141\n",
      "Smoothing factor:  100\n",
      "Total Correct Count:  1656  Total Wrong Count:  451  Accuracy:  78.5951589938301\n"
     ]
    }
   ],
   "source": [
    "smoothing_factors = [.01,0.001,.0001,1]\n",
    "for a in smoothing_factors:\n",
    "   performanceEvaluation_NB(X_train, Y_train, X_validation, Y_validation,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KlvZfEjU95dn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================Naive Bayes============================================================\n",
      "==========================FOR  1 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  98  Total Wrong Count:  10  Accuracy:  90.74074074074075\n",
      "==========================FOR  2 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  95  Total Wrong Count:  10  Accuracy:  90.47619047619048\n",
      "==========================FOR  3 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  98  Total Wrong Count:  6  Accuracy:  94.23076923076923\n",
      "==========================FOR  4 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  97  Total Wrong Count:  7  Accuracy:  93.26923076923077\n",
      "==========================FOR  5 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  98  Total Wrong Count:  7  Accuracy:  93.33333333333333\n",
      "==========================FOR  6 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  101  Total Wrong Count:  5  Accuracy:  95.28301886792453\n",
      "==========================FOR  7 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  103  Total Wrong Count:  5  Accuracy:  95.37037037037037\n",
      "==========================FOR  8 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  102  Total Wrong Count:  7  Accuracy:  93.57798165137615\n",
      "==========================FOR  9 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  102  Total Wrong Count:  7  Accuracy:  93.57798165137615\n",
      "==========================FOR  10 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  99  Total Wrong Count:  7  Accuracy:  93.39622641509433\n",
      "==========================FOR  11 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  96  Total Wrong Count:  11  Accuracy:  89.7196261682243\n",
      "==========================FOR  12 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  98  Total Wrong Count:  5  Accuracy:  95.14563106796116\n",
      "==========================FOR  13 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  102  Total Wrong Count:  7  Accuracy:  93.57798165137615\n",
      "==========================FOR  14 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  99  Total Wrong Count:  7  Accuracy:  93.39622641509433\n",
      "==========================FOR  15 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  98  Total Wrong Count:  12  Accuracy:  89.0909090909091\n",
      "==========================FOR  16 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  98  Total Wrong Count:  11  Accuracy:  89.90825688073394\n",
      "==========================FOR  17 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  95  Total Wrong Count:  14  Accuracy:  87.1559633027523\n",
      "==========================FOR  18 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  96  Total Wrong Count:  14  Accuracy:  87.27272727272727\n",
      "==========================FOR  19 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  103  Total Wrong Count:  7  Accuracy:  93.63636363636364\n",
      "==========================FOR  20 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  99  Total Wrong Count:  11  Accuracy:  90.0\n",
      "==========================FOR  21 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  96  Total Wrong Count:  14  Accuracy:  87.27272727272727\n",
      "==========================FOR  22 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  100  Total Wrong Count:  9  Accuracy:  91.74311926605505\n",
      "==========================FOR  23 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  100  Total Wrong Count:  7  Accuracy:  93.45794392523365\n",
      "==========================FOR  24 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  99  Total Wrong Count:  4  Accuracy:  96.11650485436893\n",
      "==========================FOR  25 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  98  Total Wrong Count:  9  Accuracy:  91.58878504672897\n",
      "==========================FOR  26 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  90  Total Wrong Count:  12  Accuracy:  88.23529411764706\n",
      "==========================FOR  27 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  96  Total Wrong Count:  10  Accuracy:  90.56603773584905\n",
      "==========================FOR  28 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  95  Total Wrong Count:  6  Accuracy:  94.05940594059406\n",
      "==========================FOR  29 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  104  Total Wrong Count:  6  Accuracy:  94.54545454545455\n",
      "==========================FOR  30 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  101  Total Wrong Count:  7  Accuracy:  93.51851851851852\n",
      "==========================FOR  31 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  94  Total Wrong Count:  13  Accuracy:  87.85046728971963\n",
      "==========================FOR  32 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  97  Total Wrong Count:  11  Accuracy:  89.81481481481481\n",
      "==========================FOR  33 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  95  Total Wrong Count:  13  Accuracy:  87.96296296296296\n",
      "==========================FOR  34 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  95  Total Wrong Count:  10  Accuracy:  90.47619047619048\n",
      "==========================FOR  35 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  95  Total Wrong Count:  10  Accuracy:  90.47619047619048\n",
      "==========================FOR  36 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  96  Total Wrong Count:  13  Accuracy:  88.07339449541284\n",
      "==========================FOR  37 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  99  Total Wrong Count:  8  Accuracy:  92.5233644859813\n",
      "==========================FOR  38 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  100  Total Wrong Count:  8  Accuracy:  92.5925925925926\n",
      "==========================FOR  39 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  97  Total Wrong Count:  9  Accuracy:  91.50943396226415\n",
      "==========================FOR  40 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  100  Total Wrong Count:  8  Accuracy:  92.5925925925926\n",
      "==========================FOR  41 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  98  Total Wrong Count:  9  Accuracy:  91.58878504672897\n",
      "==========================FOR  42 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  94  Total Wrong Count:  7  Accuracy:  93.06930693069307\n",
      "==========================FOR  43 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  104  Total Wrong Count:  6  Accuracy:  94.54545454545455\n",
      "==========================FOR  44 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  97  Total Wrong Count:  10  Accuracy:  90.65420560747664\n",
      "==========================FOR  45 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  97  Total Wrong Count:  10  Accuracy:  90.65420560747664\n",
      "==========================FOR  46 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  98  Total Wrong Count:  12  Accuracy:  89.0909090909091\n",
      "==========================FOR  47 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  100  Total Wrong Count:  5  Accuracy:  95.23809523809524\n",
      "==========================FOR  48 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  103  Total Wrong Count:  7  Accuracy:  93.63636363636364\n",
      "==========================FOR  49 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  100  Total Wrong Count:  9  Accuracy:  91.74311926605505\n",
      "==========================FOR  50 th iteration\n",
      "Smoothing factor:  0.01\n",
      "Total Correct Count:  97  Total Wrong Count:  9  Accuracy:  91.50943396226415\n"
     ]
    }
   ],
   "source": [
    "print(\"=========================Naive Bayes============================================================\")\n",
    "for i in range(50):\n",
    "  print(\"==========================FOR \",i+1,\"th iteration\")\n",
    "  performanceEvaluation_NB(X_train, Y_train, X_test_for_each_iteration[i], Y_test_for_each_iteration[i],0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RA_OY-TTFylh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1505031_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
